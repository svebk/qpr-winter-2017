{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation   \n",
    "The goal of this notebook is to take the inputs described below and generate a single CSV with only the information required to perform image-level processing and ad/cluster level aggregating, while performing basic sanity checks. \n",
    "\n",
    "### Inputs\n",
    "1. CP1_train_ads_labelled_fall2016.jsonl   \n",
    "This is a json lines file of ads that contain _id, class, cluster_id\n",
    "2. es_child_documents.jl   \n",
    "This is a json lines file of image objects which contain obj_stored_url and obj_parent\n",
    "3. image_url_to_valid_sha1.csv    \n",
    "This is a csv containing 2 columns: an image url (obj_stored_url), and a sha1 checksum of the file\n",
    "\n",
    "### Outputs\n",
    "1. CP1_data.csv    \n",
    "This is a csv file containing 4 columns: cluster_id, ad_id, image_sha, class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__depends__ = ['CP1_train_ads_labelled_fall2016.jsonl',\n",
    "               'es_child_documents.jl',\n",
    "               'image_url_to_valid_sha1.csv']\n",
    "__dest__ = ['map.clusterId_to_adId.pickle',\n",
    "            'map.adId_to_clusterId.pickle',\n",
    "            'map.clusterId_to_class.pickle',\n",
    "            'map.class_to_clusterId.pickle',\n",
    "            'map.adId_to_sha1s.pickle',\n",
    "            'map.sha1_to_adIds.pickle',\n",
    "            'map.cleaned.clusterId_to_adId.pickle',\n",
    "            'map.cleaned.adId_to_clusterId.pickle',\n",
    "            'map.cleaned.adId_to_sha1s.pickle',\n",
    "            'map.cleaned.sha1_to_adIds.pickle',\n",
    "            'map.cp1_data.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input file paths\n",
    "OFFICIAL_DATA_FILE = 'CP1_train_ads_labelled_fall2016.jsonl'\n",
    "ES_QUERY_RESULTS = 'es_child_documents.jl'\n",
    "IMAGE_URL_SHA1_MAP = 'image_url_to_valid_sha1.csv'\n",
    "# Mapping Cache Files\n",
    "CLUSTER_ID_TO_AD_ID = 'map.clusterId_to_adId.pickle'\n",
    "AD_ID_TO_CLUSTER_ID = 'map.adId_to_clusterId.pickle'\n",
    "CLUSTER_ID_TO_CLASS = 'map.clusterId_to_class.pickle'\n",
    "CLASS_TO_CLUSTER_ID = 'map.class_to_clusterId.pickle'\n",
    "AD_ID_TO_SHA1S = 'map.adId_to_sha1s.pickle'\n",
    "SHA1_TO_AD_IDS = 'map.sha1_to_adIds.pickle'\n",
    "# These have been stripped of image SHA1s that were labeled as both positive and negative\n",
    "CLUSTER_ID_TO_AD_ID_CLEANED = 'map.cleaned.clusterId_to_adId.pickle'\n",
    "AD_ID_TO_CLUSTER_ID_CLEANED = 'map.cleaned.adId_to_clusterId.pickle'\n",
    "AD_ID_TO_SHA1S_CLEANED = 'map.cleaned.adId_to_sha1s.pickle'\n",
    "SHA1_TO_AD_IDS_CLEANED = 'map.cleaned.sha1_to_adIds.pickle'\n",
    "# Full association mapping CSV file\n",
    "CP1_DATA_FILE = 'map.cp1_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import cPickle as pickle\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checking the official data\n",
    "\n",
    "Assumptions:   \n",
    "1) The relationship between ad_id and cluster_id is many -> 1    \n",
    "2) The relationship between cluster_id and class is 1 -> 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbreviate the data to what we need: ad ids, cluster ids, and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_id_to_ad_ids = defaultdict(set)\n",
    "ad_id_to_cluster_ids = defaultdict(set)\n",
    "\n",
    "cluster_id_to_class = defaultdict(set)\n",
    "class_to_cluster_id = defaultdict(set)\n",
    "\n",
    "if (os.path.isfile(CLUSTER_ID_TO_AD_ID) and \n",
    "        os.path.isfile(AD_ID_TO_CLUSTER_ID) and\n",
    "        os.path.isfile(CLUSTER_ID_TO_CLASS) and\n",
    "        os.path.isfile(CLASS_TO_CLUSTER_ID)):\n",
    "    print \"Loading existing intermediate results\"\n",
    "    with open(CLUSTER_ID_TO_AD_ID) as f:\n",
    "        cluster_id_to_ad_ids = pickle.load(f)\n",
    "    with open(AD_ID_TO_CLUSTER_ID) as f:\n",
    "        ad_id_to_cluster_ids = pickle.load(f)\n",
    "    with open(CLUSTER_ID_TO_CLASS) as f:\n",
    "        cluster_id_to_class = pickle.load(f)\n",
    "    with open(CLASS_TO_CLUSTER_ID) as f:\n",
    "        class_to_cluster_id = pickle.load(f)\n",
    "else:\n",
    "    print \"Extracting relevant info from JSONL\"\n",
    "    with open(OFFICIAL_DATA_FILE) as infile:\n",
    "        for line in infile:\n",
    "            document = json.loads(line.strip())\n",
    "\n",
    "            cluster_id_to_ad_ids[document['cluster_id']].add(document['_id'])\n",
    "            ad_id_to_cluster_ids[document['_id']].add(document['cluster_id'])\n",
    "            cluster_id_to_class[document['cluster_id']].add(document['class'])\n",
    "            class_to_cluster_id[document['class']].add(document['cluster_id'])\n",
    "            \n",
    "    print \"Saving intermediate results\"\n",
    "    with open(CLUSTER_ID_TO_AD_ID, 'wb') as f:\n",
    "        pickle.dump(cluster_id_to_ad_ids, f, -1)\n",
    "    with open(AD_ID_TO_CLUSTER_ID, 'wb') as f:\n",
    "        pickle.dump(ad_id_to_cluster_ids, f, -1)\n",
    "    with open(CLUSTER_ID_TO_CLASS, 'wb') as f:\n",
    "        pickle.dump(cluster_id_to_class, f, -1)\n",
    "    with open(CLASS_TO_CLUSTER_ID, 'wb') as f:\n",
    "        pickle.dump(class_to_cluster_id, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity check that each cluster has at least one ad\n",
    "for cluster_id, ad_ids in cluster_id_to_ad_ids.iteritems():\n",
    "    assert len(ad_ids) > 0\n",
    "    \n",
    "# Sanity check no ad falls in more than one cluster (assumption 1)\n",
    "all_ad_ids = []\n",
    "num_unique_ad_ids = 0\n",
    "\n",
    "for _, ad_ids in cluster_id_to_ad_ids.iteritems():\n",
    "    all_ad_ids += list(ad_ids)\n",
    "    num_unique_ad_ids += len(ad_ids)\n",
    "    \n",
    "assert len(all_ad_ids) == num_unique_ad_ids\n",
    "\n",
    "# Sanity check that each cluster only belongs to one class (assumption 2) \n",
    "for _, cls in cluster_id_to_class.iteritems():\n",
    "    assert len(cls) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Official data descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print '%d clusters (%d positive, %d negative)' % (len(cluster_id_to_class),\n",
    "                                                  len([x for x in cluster_id_to_class.values() if x == {1}]),\n",
    "                                                  len([x for x in cluster_id_to_class.values() if x == {0}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ads_per_positive_cluster = [len(ad_ids) for cid, ad_ids in cluster_id_to_ad_ids.iteritems() \\\n",
    "                            if cluster_id_to_class[cid] == {1}]\n",
    "print 'min/med/avg/max/total ads per positive cluster: %d/%d/%d/%d/%d' % (min(ads_per_positive_cluster),\n",
    "                                                                          np.median(ads_per_positive_cluster),\n",
    "                                                                          np.average(ads_per_positive_cluster),\n",
    "                                                                          max(ads_per_positive_cluster),\n",
    "                                                                          sum(ads_per_positive_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ads_per_negative_cluster = [len(ad_ids) for cid, ad_ids in cluster_id_to_ad_ids.iteritems() \\\n",
    "                            if cluster_id_to_class[cid] == {0}]\n",
    "print 'min/med/avg/max/total ads per negative cluster: %d/%d/%d/%d/%d' % (min(ads_per_negative_cluster),\n",
    "                                                                          np.median(ads_per_negative_cluster),\n",
    "                                                                          np.average(ads_per_negative_cluster),\n",
    "                                                                          max(ads_per_negative_cluster),\n",
    "                                                                          sum(ads_per_negative_cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associating Imagery\n",
    "\n",
    "The shas present here have already been vetted by SMQTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(AD_ID_TO_SHA1S) and os.path.isfile(SHA1_TO_AD_IDS):\n",
    "    print \"Loading cached maps\"\n",
    "    ad_id_to_shas = pickle.load(open(AD_ID_TO_SHA1S))\n",
    "    sha_to_ad_ids = pickle.load(open(SHA1_TO_AD_IDS))\n",
    "else:\n",
    "    ad_id_to_shas = defaultdict(set)\n",
    "    sha_to_ad_ids = defaultdict(set)\n",
    "    ad_id_to_image_urls = defaultdict(set)\n",
    "    image_url_to_ad_ids = defaultdict(set)\n",
    "\n",
    "    print 'Read adId->imageSha relationships from ElasticSearch query results'\n",
    "    with open(ES_QUERY_RESULTS) as infile:\n",
    "        for line in infile:\n",
    "            document = json.loads(line.strip())\n",
    "\n",
    "            if isinstance(document['obj_parent'], list):\n",
    "                ad_ids = document['obj_parent']\n",
    "            else:\n",
    "                ad_ids = [document['obj_parent']]\n",
    "\n",
    "            for ad_id in ad_ids:\n",
    "                if document['obj_stored_url']:\n",
    "                    ad_id_to_image_urls[ad_id].add(document['obj_stored_url'])\n",
    "                    image_url_to_ad_ids[document['obj_stored_url']].add(ad_id)\n",
    "\n",
    "    print 'Read in [URL,SHA1] pairs for computable images'\n",
    "    image_url_to_sha = {}\n",
    "    image_sha_to_url = {}\n",
    "    with open(IMAGE_URL_SHA1_MAP) as infile:\n",
    "        for (image_url, sha1) in csv.reader(infile):\n",
    "            image_url_to_sha[image_url] = sha1\n",
    "            image_sha_to_url[sha1] = image_url\n",
    "\n",
    "    print 'Construct map of ad ID to image SHA1s in that ad'\n",
    "    for (ad_id, image_urls) in ad_id_to_image_urls.iteritems():\n",
    "        try:\n",
    "            ad_image_shas = set([image_url_to_sha[url] for url in image_urls])\n",
    "            ad_id_to_shas[ad_id] = ad_image_shas\n",
    "            for sha in ad_image_shas:\n",
    "                sha_to_ad_ids[sha].add(ad_id)\n",
    "        except KeyError:\n",
    "            # There might not be a sha1 for the image url since some shas were invalid (from SMQTK) \n",
    "            pass\n",
    "    \n",
    "    print 'Save mappings'\n",
    "    with open(AD_ID_TO_SHA1S, 'wb') as f:\n",
    "        pickle.dump(ad_id_to_shas, f, -1)\n",
    "    with open(SHA1_TO_AD_IDS, 'wb') as f:\n",
    "        pickle.dump(sha_to_ad_ids, f, -1)\n",
    "        \n",
    "    del ad_id_to_image_urls, image_url_to_ad_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sanity check that each ad has at least 1 sha\n",
    "for shas in ad_id_to_shas.values():\n",
    "    assert len(shas) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shas_per_ad = map(len, ad_id_to_shas.values())\n",
    "print 'min/med/avg/max/total images per ad: %d/%d/%d/%d/%d' % (min(shas_per_ad),\n",
    "                                                               np.median(shas_per_ad),\n",
    "                                                               np.average(shas_per_ad),\n",
    "                                                               max(shas_per_ad),\n",
    "                                                               sum(shas_per_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ad_per_sha = map(len, ad_id_to_shas.values())\n",
    "print 'min/med/avg/max/total ads per SHA1: %d/%d/%d/%d/%d' % (min(ad_per_sha),\n",
    "                                                               np.median(ad_per_sha),\n",
    "                                                               np.average(ad_per_sha),\n",
    "                                                               max(ad_per_sha),\n",
    "                                                               sum(ad_per_sha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity check: All ads have at least one image\n",
    "c = 0\n",
    "for ad, shas in ad_id_to_shas.iteritems():\n",
    "    if len(shas) == 0:\n",
    "        assert \"Ad with no images:\", ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find shas that are marked positive and negative\n",
    "sha_to_class = defaultdict(set)\n",
    "        \n",
    "for sha, parent_ads in sha_to_ad_ids.iteritems():\n",
    "    sha_classes = set()\n",
    "    for ad_id in parent_ads:\n",
    "        for cluster_id in ad_id_to_cluster_ids[ad_id]:\n",
    "            sha_classes.update(cluster_id_to_class[cluster_id])\n",
    "    sha_to_class[sha] = sha_classes\n",
    "\n",
    "bad_shas = set([sha for sha, classes in sha_to_class.iteritems() if len(classes) > 1])\n",
    "\n",
    "print len(bad_shas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"copy original mappings\"\n",
    "cluster_id_to_ad_ids_cleaned = pickle.load(open(CLUSTER_ID_TO_AD_ID))\n",
    "ad_id_to_cluster_ids_cleaned = pickle.load(open(AD_ID_TO_CLUSTER_ID))\n",
    "ad_id_to_shas_cleaned = pickle.load(open(AD_ID_TO_SHA1S))\n",
    "sha_to_ad_ids_cleaned = pickle.load(open(SHA1_TO_AD_IDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "empty_ads = set()\n",
    "empty_clusters = set()\n",
    "\n",
    "print \"Clearing SHA1 from referent ads\"\n",
    "for bad_sha in bad_shas:\n",
    "    parent_ads = sha_to_ad_ids_cleaned[bad_sha]\n",
    "    for p_ad in parent_ads:\n",
    "        ad_id_to_shas_cleaned[p_ad].remove(bad_sha)\n",
    "        # check for empty adIds\n",
    "        if len(ad_id_to_shas_cleaned[p_ad]) == 0:\n",
    "            empty_ads.add(p_ad)\n",
    "            del ad_id_to_shas_cleaned[p_ad]\n",
    "    del sha_to_ad_ids_cleaned[bad_sha]\n",
    "    \n",
    "print \"Clearing empty ads from referent clusters\"\n",
    "for e_ad in empty_ads:\n",
    "    parent_clusters = ad_id_to_cluster_ids_cleaned[e_ad]\n",
    "    for p_cluster in parent_clusters:\n",
    "        cluster_id_to_ad_ids_cleaned[p_cluster].remove(e_ad)\n",
    "        # Check for empty clusters\n",
    "        if len(cluster_id_to_ad_ids_cleaned) == 0:\n",
    "            empty_clusters.add(p_cluster)\n",
    "            del cluster_id_to_ad_ids_cleaned[p_cluster]\n",
    "    del ad_id_to_cluster_ids_cleaned[e_ad]\n",
    "\n",
    "print\n",
    "print \"New empty ads:\", len(empty_ads)\n",
    "print \"New empty clusters:\", len(empty_clusters)\n",
    "print\n",
    "print 'original ads:', len(ad_id_to_shas), len(ad_id_to_cluster_ids)\n",
    "print 'cleaned ads :', len(ad_id_to_shas_cleaned), len(ad_id_to_cluster_ids_cleaned)\n",
    "print\n",
    "print 'original clusters:', len(cluster_id_to_ad_ids)\n",
    "print 'cleaned clusters :', len(cluster_id_to_ad_ids_cleaned)\n",
    "    \n",
    "with open(AD_ID_TO_SHA1S_CLEANED, 'wb') as f:\n",
    "    pickle.dump(ad_id_to_shas_cleaned, f, -1)\n",
    "with open(SHA1_TO_AD_IDS_CLEANED, 'wb') as f:\n",
    "    pickle.dump(sha_to_ad_ids_cleaned, f, -1)\n",
    "with open(CLUSTER_ID_TO_AD_ID_CLEANED, 'wb') as f:\n",
    "    pickle.dump(cluster_id_to_ad_ids_cleaned, f, -1)\n",
    "with open(AD_ID_TO_CLUSTER_ID_CLEANED, 'wb') as f:\n",
    "    pickle.dump(ad_id_to_cluster_ids_cleaned, f, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one CSV with all the relevant information, in the format of:    \n",
    "cluster_id, ad_id, image_sha, class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(CP1_DATA_FILE, 'w') as outfile:\n",
    "    writer = csv.writer(outfile, lineterminator='\\n')\n",
    "    \n",
    "    for (cluster_id, ad_ids) in cluster_id_to_ad_ids_cleaned.iteritems():\n",
    "        for ad_id in ad_ids:\n",
    "            for image_sha in ad_id_to_shas_cleaned[ad_id]:\n",
    "                writer.writerow([cluster_id, ad_id, image_sha, list(cluster_id_to_class[cluster_id])[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print clusters ordered by number of images\n",
    "clusters = defaultdict(set)\n",
    "\n",
    "with open(CP1_DATA_FILE) as infile:\n",
    "    for (cid, ad_id, sha, cls) in csv.reader(infile):\n",
    "        clusters[cid].add(sha)\n",
    "\n",
    "clusters_by_size = sorted(clusters.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "for (cluster_id, shas) in clusters_by_size:\n",
    "    print '%s %s %d' % (cluster_id, cluster_id_to_class[cluster_id], len(shas))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
